# ğŸŸ¥ Reddit ETL Data Engineering on AWS â˜ï¸

[![Python](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/)
[![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.7-orange)](https://airflow.apache.org/)
[![AWS](https://img.shields.io/badge/AWS-S3%2C%20IAM%2C%20EC2-lightgrey)](https://aws.amazon.com/)

Python â€¢ Airflow â€¢ AWS â€¢ Reddit

---

## ğŸ”¹ Overview

This repository contains a complete end-to-end ETL pipeline that extracts Reddit posts, processes the data, and stores it in AWS S3, orchestrated using Apache Airflow. The project is designed for **real-world scalable deployment** and integrates fully with AWS services.

---

## ğŸ”¹ Key Features

- ğŸ“ Extracts Reddit posts using PRAW (Python Reddit API Wrapper)  
- ğŸ”„ ETL pipeline for transforming and cleaning Reddit data  
- â˜ï¸ Uploads processed data to AWS S3  
- â±ï¸ Uses Airflow DAGs for scheduling and orchestration  
- ğŸ³ Dockerized environment for easy deployment  
- ğŸ” Full integration with AWS services: S3, IAM, EC2, Glue Crawlers, CloudWatch  
- âš™ï¸ Configurable via a single `config.conf` file  

---

## ğŸ“‚ Repository Structure

RedditDataEngineering/
â”œâ”€â”€ dags/ Airflow DAGs and ETL scripts
â”‚ â”œâ”€â”€ etl_reddit_pipeline.py
â”‚ â”œâ”€â”€ reddit_dag.py
â”‚ â”œâ”€â”€ reddit_extraction.py
â”‚ â””â”€â”€ utils/constants.py
â”œâ”€â”€ etls/ AWS ETL scripts
â”‚ â”œâ”€â”€ aws_etl.py
â”‚ â””â”€â”€ reddit_etl.py
â”œâ”€â”€ pipelines/ Modular pipelines
â”‚ â”œâ”€â”€ aws_s3_pipeline.py
â”‚ â””â”€â”€ reddit_pipeline.py
â”œâ”€â”€ data/ Input/output data
â”‚ â”œâ”€â”€ input/
â”‚ â””â”€â”€ output/
â”œâ”€â”€ config/ Configuration
â”‚ â””â”€â”€ config.conf # Excluded in .gitignore (contains secrets)
â”œâ”€â”€ Dockerfile Docker setup
â”œâ”€â”€ docker-compose.yml Docker Compose for Airflow
â”œâ”€â”€ requirements.txt Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

**Note:** Sensitive files like `config/config.conf`, `.venv/`, `logs/`, `__pycache__/`, and `data/output/` are excluded via `.gitignore`.

---

## ğŸš€ Visual Pipeline Diagram

+-----------------+
| Reddit API |
| (PRAW) |
+--------+--------+
|
v
+-----------------+
| ETL Scripts |
| (Extraction, |
| Transformation,|
| Loading) |
+--------+--------+
|
v
+-----------------+
| AWS S3 Bucket |
| (Processed CSV)|
+--------+--------+
|
v
+-----------------+
| Apache Airflow |
| DAG Scheduler |
+-----------------+

**Explanation:**

- ğŸŸ¥ Reddit API fetches posts using PRAW  
- ğŸ”„ ETL scripts clean, transform, and prepare the data  
- â˜ï¸ Data is uploaded to AWS S3 bucket  
- â±ï¸ Airflow orchestrates, schedules, and monitors the ETL pipeline  

---

## ğŸ³ Running the Project

1. **Start Docker Compose**
```bash
docker-compose up -d

Access Airflow Web UI
Open your browser: http://localhost:8080
DAGs are available under Reddit ETL Pipeline
Trigger manually or wait for scheduled runs
Monitor Logs & Output
Processed CSVs saved to data/output/
Automatically uploaded to your AWS S3 bucket

ğŸ“¦ Python Dependencies
pandas â€“ Data manipulation
boto3 â€“ AWS S3 integration
praw â€“ Reddit API
apache-airflow â€“ Workflow orchestration
python-dotenv â€“ Environment variable management
Install all dependencies:
pip install -r requirements.txt

ğŸ“ License
MIT License
